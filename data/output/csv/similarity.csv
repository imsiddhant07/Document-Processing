ref sentence,ref citation,semantic score,lexical score
"The quality, randomness, bias, inconsistencies in peer reviews is well-debated across the academic community (Bornmann and Daniel, 2010)",Reliability of reviewers' ratings when using public peer review: a case study,0.8100943244938387,0.03571428571428571
" To study the arbitrariness inherent in the existing peer review system, organisers of the NIPS 2014 conference assigned 10% submissions to two different sets of reviewers and observed that the two committees disagreed for more than quarter of the papers (Langford and Guzdial, 2015)","The arbitrariness of reviews, and advice for school administrators",0.8824015716934207,0.1282051282051282
" Many are of the opinion that the existing peer review system is fragile as it only depends on the view of a selected few (Smith, 2006)",Peer review: a flawed process at the heart of science and journals,0.8930173158558236,0.12903225806451613
"The PeerRead dataset (Kang et  , 2018) is an excellent resource towards research and study on this very impactful and crucial problem","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.8528770841162835,0.06666666666666667
 Price and Flach (2017) did a thorough study of the various means of computational support to the peer review system,Computational support for academic peer review: a perspective from artificial intelligence,0.8783195362885826,0.16
 Mrowinski et   (2017) explored an evolutionary algorithm to improve editorial strategies in peer review,Artificial intelligence in peer review: How can evolutionary computation support journal editors?,0.8650046017468194,0.13043478260869565
" The famous Toronto Paper Matching system (Charlin and Zemel, 2013) was developed to match paper with reviewers",The toronto paper matching system: an automated paperreviewer assignment system,0.8109941838911933,0.23809523809523808
" Recently we (Ghosal et  , 2018b,a) investigated the impact of various fea-tures in the editorial pre-screening process",,,
 Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts,Sentiment Analysis of Peer Review Texts for Scholarly Papers,0.8350493802874618,0.3
" We carry our current investigations on a portion of the recently released PeerRead dataset (Kang et  , 2018)","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.8037801865708459,0.11538461538461539
 Our approach achieves significant performance improvement over the two tasks defined in Kang et   (2018),"A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.7690538776933182,0.0
" For more details on the dataset creation and the task, we request the readers to refer to Kang et   (2018)","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.8186400602942387,0.07692307692307693
" One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in Kang et   (2018)","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.8222573035184341,0.018518518518518517
" This also seconds our recent finding that determining the scope or appropriateness of an article to a venue is the first essential step in peer review (Ghosal et  , 2018a)",Investigating domain features for scope detection and classification of scientific articles,0.8221546971698572,0.05263157894736842
" To calculate the sentiment polarity of a review text, we take the average of the sentence wise sentiment scores from Valence Aware Dictionary and sEntiment Reasoner (VADER) (Hutto and Gilbert, 2014)",VADER: A parsimonious rule-based model for sentiment analysis of social media text,0.8245933960487226,0.08823529411764706
"com/allenai/science-parse Encoder (USE) (Cer et  , 2018), d is the dimension of the sentence semantic vector which is 512",Universal Sentence Encoder for English,0.6429567698172837,0.1
"To compare with Kang et   (2018), we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.7876354311074513,0.030303030303030304
" However, Kang et   (2018) performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.8107394492810657,0.10344827586206896
" With only using review+sentiment information, we are still able to outperform Kang et   (2018) by a margin of 11% in terms of RMSE","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.7982331749125998,0.06451612903225806
"For Task 2, we observe that the handcrafted feature-based system by Kang et   (2018) performs inferior compared to the baselines","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.7832547929323834,0.0
 The reason is that the work reported in Kang et   (2018) relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture,"A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.8292905711764477,0.0
" However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in Kang et   (2018) for ACL 2017","A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications",0.766495085677473,0.0
Figure 3 shows the output activations 5 from the final layer of MLP Senti against the predicted recommendation scores,,,
