{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree1 = ET.parse(\"/home/siddhant/Projects/have-a-look-at-my-paper/data/output/xml/P19-1106.tei.xml\")\n",
    "root1 = tree1.getroot()\n",
    "\n",
    "tree2 = ET.parse(\"/home/siddhant/Projects/have-a-look-at-my-paper/data/output/xml/210603714.tei.xml\")\n",
    "root2 = tree2.getroot()\n",
    "\n",
    "tree3 = ET.parse(\"/home/siddhant/Projects/have-a-look-at-my-paper/data/output/xml/210503404.tei.xml\")\n",
    "root3 = tree3.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "parsed_file = BeautifulSoup(open(\"/home/siddhant/Projects/have-a-look-at-my-paper/data/output/xml/P19-1106.tei.xml\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_authors(article):\n",
    "    \"\"\"\n",
    "    Parse authors from a given BeautifulSoup of an article\n",
    "    \"\"\"\n",
    "    author_names = article.find(\"sourcedesc\").findAll(\"persname\")\n",
    "    authors = []\n",
    "    for author in author_names:\n",
    "        firstname = author.find(\"forename\", {\"type\": \"first\"})\n",
    "        firstname = firstname.text.strip() if firstname is not None else \"\"\n",
    "        middlename = author.find(\"forename\", {\"type\": \"middle\"})\n",
    "        middlename = middlename.text.strip() if middlename is not None else \"\"\n",
    "        lastname = author.find(\"surname\")\n",
    "        lastname = lastname.text.strip() if lastname is not None else \"\"\n",
    "        if middlename is not \"\":\n",
    "            authors.append(firstname + \" \" + middlename + \" \" + lastname)\n",
    "        else:\n",
    "            authors.append(firstname + \" \" + lastname)\n",
    "    authors = \"; \".join(authors)\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tirthankar Ghosal; Rajeev Verma; Asif Ekbal; Pushpak Bhattacharyya\n"
     ]
    }
   ],
   "source": [
    "auth = parse_authors(parsed_file)\n",
    "print(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(article):\n",
    "    \"\"\"\n",
    "    Parse date from a given BeautifulSoup of an article\n",
    "    \"\"\"\n",
    "    pub_date = article.find(\"publicationstmt\")\n",
    "    year = pub_date.find(\"date\")\n",
    "    year = year.attrs.get(\"when\") if year is not None else \"\"\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n"
     ]
    }
   ],
   "source": [
    "y = parse_date(parsed_file)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_abstract(article):\n",
    "    \"\"\"\n",
    "    Parse abstract from a given BeautifulSoup of an article\n",
    "    \"\"\"\n",
    "    div = article.find(\"abstract\")\n",
    "    abstract = \"\"\n",
    "    for p in list(div.children):\n",
    "        if not isinstance(p, NavigableString) and len(list(p)) > 0:\n",
    "            abstract += \" \".join(\n",
    "                [elem.text for elem in p if not isinstance(elem, NavigableString)]\n",
    "            )\n",
    "    return abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically validating a research artefact is one of the frontiers in Artificial Intelligence (AI) that directly brings it close to competing with human intellect and intuition. Although criticized sometimes, the existing peer review system still stands as the benchmark of research validation. The present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of AI. However, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. Here in this work, we investigate the role of reviewers sentiments embedded within peer review texts to predict the peer review outcome. Our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and the review polarity to predict the overall recommendation score as well as the final decision. We achieve significant performance improvement over the baselines (âˆ¼ 29% error reduction) proposed in a recently released dataset of peer reviews. An AI of this kind could assist the editors/program chairs as an additional layer of confidence in the final decision making, especially when non-responding/missing reviewers are frequent in present day peer review.\n"
     ]
    }
   ],
   "source": [
    "ab = parse_abstract(parsed_file)\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_sections(article, as_list: bool = False):\n",
    "    \"\"\"\n",
    "    Parse list of sections from a given BeautifulSoup of an article\n",
    "    Parameters\n",
    "    ==========\n",
    "    as_list: bool, if True, output text as a list of paragraph instead\n",
    "        of joining it together as one single text\n",
    "    \"\"\"\n",
    "    article_text = article.find(\"text\")\n",
    "    divs = article_text.find_all(\"div\", attrs={\"xmlns\": \"http://www.tei-c.org/ns/1.0\"})\n",
    "    sections = []\n",
    "    for div in divs:\n",
    "        div_list = list(div.children)\n",
    "        if len(div_list) == 0:\n",
    "            heading = \"\"\n",
    "            text = \"\"\n",
    "        elif len(div_list) == 1:\n",
    "            if isinstance(div_list[0], NavigableString):\n",
    "                heading = str(div_list[0])\n",
    "                text = \"\"\n",
    "            else:\n",
    "                heading = \"\"\n",
    "                text = div_list[0].text\n",
    "        else:\n",
    "            text = []\n",
    "            heading = div_list[0]\n",
    "            if isinstance(heading, NavigableString):\n",
    "                heading = str(heading)\n",
    "                p_all = list(div.children)[1:]\n",
    "            else:\n",
    "                heading = \"\"\n",
    "                p_all = list(div.children)\n",
    "            for p in p_all:\n",
    "                if p is not None:\n",
    "                    try:\n",
    "                        text.append(p.text)\n",
    "                    except:\n",
    "                        pass\n",
    "            if not False:\n",
    "                text = \"\\n\".join(text)\n",
    "        if heading is not \"\" or text is not \"\":\n",
    "            ref_dict = calculate_number_of_references(div)\n",
    "            sections.append(\n",
    "                {\n",
    "                    \"heading\": heading,\n",
    "                    \"text\": text,\n",
    "                    \"n_publication_ref\": ref_dict[\"n_publication_ref\"],\n",
    "                    \"n_figure_ref\": ref_dict[\"n_figure_ref\"],\n",
    "                }\n",
    "            )\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_references(div):\n",
    "    \"\"\"\n",
    "    For a given section, calculate number of references made in the section\n",
    "    \"\"\"\n",
    "    n_publication_ref = len(\n",
    "        [ref for ref in div.find_all(\"ref\") if ref.attrs.get(\"type\") == \"bibr\"]\n",
    "    )\n",
    "    n_figure_ref = len(\n",
    "        [ref for ref in div.find_all(\"ref\") if ref.attrs.get(\"type\") == \"figure\"]\n",
    "    )\n",
    "    return {\"n_publication_ref\": n_publication_ref, \"n_figure_ref\": n_figure_ref}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rapid increase in research article submissions across different venues is posing a significant management challenge for the journal editors and conference program chairs 1 . Among the load of works like assigning reviewers, ensuring timely receipt of reviews, slot-filling against the non-responding reviewer, taking informed decisions, communicating to the authors, etc., editors/program chairs are usually overwhelmed with many such demanding yet crucial tasks. However, the major hurdle lies in to decide the acceptance and rejection of the manuscripts based on the reviews received from the reviewers.\n",
      "The quality, randomness, bias, inconsistencies in peer reviews is well-debated across the academic community (Bornmann and Daniel, 2010). Due to the rise in article submissions and nonavailability of expert reviewers, editors/program chairs are sometimes left with no other options than to assign papers to the novice, out of domain reviewers which sometimes results in more inconsistencies and poor quality reviews. To study the arbitrariness inherent in the existing peer review system, organisers of the NIPS 2014 conference assigned 10% submissions to two different sets of reviewers and observed that the two committees disagreed for more than quarter of the papers (Langford and Guzdial, 2015). Again it is quite common that a paper rejected in one venue gets the cut in another with little or almost no improvement in quality. Many are of the opinion that the existing peer review system is fragile as it only depends on the view of a selected few (Smith, 2006). Moreover, even a preliminary study into the inners of the peer review system is itself very difficult because of data confidentiality and copyright issues of the publishers. However, the silver lining is that the peer review system is evolving with the likes of OpenReviews 2 , author response periods/rebuttals, increased effective communications between authors and reviewers, open access initiatives, peer review workshops, review forms with objective questionnaires, etc. gaining momentum.\n",
      "The PeerRead dataset (Kang et al., 2018) is an excellent resource towards research and study on this very impactful and crucial problem. With our ongoing effort towards the development of an Artificial Intelligence (AI)-assisted peer review system, we are intrigued with: What if there is an additional AI reviewer which predicts decisions by learning the high-level interplay between the review texts and the papers? How would the sentiment embedded within the review texts empower such decision-making? Although editors/program chairs usually go by the majority of the reviewer recommendations, they still need to go through all the review texts corresponding to all the submissions. A good use case of this research would be: slot-filling the missing reviewer, providing an additional perspective to the editor in cases of contrasting/borderline reviews. This work in no way attempts to replace the human reviewers; instead, we are intrigued to see how an AI can act as an additional reviewer with inputs from her human counterparts and aid the decision-making in the peer review process.\n",
      "We develop a deep neural architecture incorporating full paper information and review text along with the associated sentiment to predict the acceptability and recommendation score of a given research article. We perform two tasks, a classification (predicting accept/reject decision) and a regression (predicting recommendation score) one. The evaluation shows that our proposed model successfully outperforms the earlier reported results in PeerRead. We also show that the addition of review sentiment component significantly enhances the predictive capability of such a system.\n"
     ]
    }
   ],
   "source": [
    "se = parse_sections(parsed_file)\n",
    "print(se[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_sentences(article):\n",
    "    article_text = article.find(\"text\")\n",
    "    divs = article_text.find_all(\"div\", attrs={\"xmlns\": \"http://www.tei-c.org/ns/1.0\"})\n",
    "    sections = []\n",
    "    for div in divs:\n",
    "        for p in div.find_all(\"p\"):\n",
    "            sentences = p.text.replace(\"al.\", \" \").split(\".\")\n",
    "            for ref in p.find_all(\"ref\"):\n",
    "                if ref.attrs.get(\"type\") == \"bibr\":\n",
    "                    ref_text = ref.text\n",
    "                    ref_id = ref.attrs.get(\"target\")\n",
    "                    ref_sentences = []\n",
    "                    \n",
    "                    for sentence in sentences:\n",
    "                        if ref_text[:7] in sentence and len(ref_sentences)<1:\n",
    "                            ref_sentences.append(sentence)\n",
    "                            sentences.remove(sentence)\n",
    "                    sections.append(\n",
    "                        {\n",
    "                            \"ref_id\": ref_id,\n",
    "                            \"ref_text\": ref_text,\n",
    "                            \"ref_sentences\": ref_sentences,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b0', 'ref_text': '(Bornmann and Daniel, 2010)', 'ref_sentences': ['The quality, randomness, bias, inconsistencies in peer reviews is well-debated across the academic community (Bornmann and Daniel, 2010)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b8', 'ref_text': '(Langford and Guzdial, 2015)', 'ref_sentences': [' To study the arbitrariness inherent in the existing peer review system, organisers of the NIPS 2014 conference assigned 10% submissions to two different sets of reviewers and observed that the two committees disagreed for more than quarter of the papers (Langford and Guzdial, 2015)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b11', 'ref_text': '(Smith, 2006)', 'ref_sentences': [' Many are of the opinion that the existing peer review system is fragile as it only depends on the view of a selected few (Smith, 2006)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': '(Kang et al., 2018)', 'ref_sentences': ['The PeerRead dataset (Kang et  , 2018) is an excellent resource towards research and study on this very impactful and crucial problem']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b10', 'ref_text': 'Price and Flach (2017)', 'ref_sentences': [' Price and Flach (2017) did a thorough study of the various means of computational support to the peer review system']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b9', 'ref_text': 'Mrowinski et al. (2017)', 'ref_sentences': [' Mrowinski et   (2017) explored an evolutionary algorithm to improve editorial strategies in peer review']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b2', 'ref_text': '(Charlin and Zemel, 2013)', 'ref_sentences': [' The famous Toronto Paper Matching system (Charlin and Zemel, 2013) was developed to match paper with reviewers']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': None, 'ref_text': '(Ghosal et al., 2018b,a)', 'ref_sentences': [' Recently we (Ghosal et  , 2018b,a) investigated the impact of various fea-tures in the editorial pre-screening process']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b12', 'ref_text': 'Wang and Wan (2018)', 'ref_sentences': [' Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': '(Kang et al., 2018)', 'ref_sentences': [' We carry our current investigations on a portion of the recently released PeerRead dataset (Kang et  , 2018)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' Our approach achieves significant performance improvement over the two tasks defined in Kang et   (2018)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' For more details on the dataset creation and the task, we request the readers to refer to Kang et   (2018)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in Kang et   (2018)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b3', 'ref_text': '(Ghosal et al., 2018a)', 'ref_sentences': [' This also seconds our recent finding that determining the scope or appropriateness of an article to a venue is the first essential step in peer review (Ghosal et  , 2018a)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b6', 'ref_text': '(Hutto and Gilbert, 2014)', 'ref_sentences': [' To calculate the sentiment polarity of a review text, we take the average of the sentence wise sentiment scores from Valence Aware Dictionary and sEntiment Reasoner (VADER) (Hutto and Gilbert, 2014)']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b1', 'ref_text': '(Cer et al., 2018)', 'ref_sentences': ['com/allenai/science-parse Encoder (USE) (Cer et  , 2018), d is the dimension of the sentence semantic vector which is 512']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': ['To compare with Kang et   (2018), we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' However, Kang et   (2018) performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' With only using review+sentiment information, we are still able to outperform Kang et   (2018) by a margin of 11% in terms of RMSE']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': ['For Task 2, we observe that the handcrafted feature-based system by Kang et   (2018) performs inferior compared to the baselines']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' The reason is that the work reported in Kang et   (2018) relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': '#b7', 'ref_text': 'Kang et al. (2018)', 'ref_sentences': [' However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in Kang et   (2018) for ACL 2017']}\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "{'ref_id': None, 'ref_text': '5', 'ref_sentences': ['Figure 3 shows the output activations 5 from the final layer of MLP Senti against the predicted recommendation scores']}\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sec = ref_sentences(parsed_file)\n",
    "print(len(sec))\n",
    "for s in sec:\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(s)\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_references(article):\n",
    "    \"\"\"\n",
    "    Parse list of references from a given BeautifulSoup of an article\n",
    "    \"\"\"\n",
    "    reference_list = []\n",
    "    references = article.find(\"text\").find(\"div\", attrs={\"type\": \"references\"})\n",
    "    references = references.find_all(\"biblstruct\") if references is not None else []\n",
    "    reference_list = []\n",
    "    for reference in references:\n",
    "        title = reference.find(\"title\", attrs={\"level\": \"a\"})\n",
    "        if title is None:\n",
    "            title = reference.find(\"title\", attrs={\"level\": \"m\"})\n",
    "        title = title.text if title is not None else \"\"\n",
    "        journal = reference.find(\"title\", attrs={\"level\": \"j\"})\n",
    "        journal = journal.text if journal is not None else \"\"\n",
    "        if journal is \"\":\n",
    "            journal = reference.find(\"publisher\")\n",
    "            journal = journal.text if journal is not None else \"\"\n",
    "        year = reference.find(\"date\")\n",
    "        year = year.attrs.get(\"when\") if year is not None else \"\"\n",
    "        authors = []\n",
    "        for author in reference.find_all(\"author\"):\n",
    "            firstname = author.find(\"forename\", {\"type\": \"first\"})\n",
    "            firstname = firstname.text.strip() if firstname is not None else \"\"\n",
    "            middlename = author.find(\"forename\", {\"type\": \"middle\"})\n",
    "            middlename = middlename.text.strip() if middlename is not None else \"\"\n",
    "            lastname = author.find(\"surname\")\n",
    "            lastname = lastname.text.strip() if lastname is not None else \"\"\n",
    "            if middlename is not \"\":\n",
    "                authors.append(firstname + \" \" + middlename + \" \" + lastname)\n",
    "            else:\n",
    "                authors.append(firstname + \" \" + lastname)\n",
    "        authors = \"; \".join(authors)\n",
    "        reference_list.append(\n",
    "            {\"title\": title, \"journal\": journal, \"year\": year, \"authors\": authors}\n",
    "        )\n",
    "    return reference_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"Reliability of reviewers' ratings when using public peer review: a case study\", 'journal': 'Learned Publishing', 'year': '2010-04', 'authors': 'Lutz Bornmann; H-D Daniel'}\n"
     ]
    }
   ],
   "source": [
    "ref = parse_references(parsed_file)\n",
    "print(ref[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in root1:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in root2:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://www.tei-c.org/ns/1.0}teiHeader {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://www.tei-c.org/ns/1.0}text {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n"
     ]
    }
   ],
   "source": [
    "for child in root3:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for child in root1.findall(\"{http://www.tei-c.org/ns/1.0}teiHeader\"):\n",
    "    fileDesc = child.find(\"{http://www.tei-c.org/ns/1.0}fileDesc\")\n",
    "    title = fileDesc.find(\"{http://www.tei-c.org/ns/1.0}titleStmt\").find(\"{http://www.tei-c.org/ns/1.0}title\").text\n",
    "    publisher = fileDesc.find(\"{http://www.tei-c.org/ns/1.0}publicationStmt\").find(\"{http://www.tei-c.org/ns/1.0}publisher\").text\n",
    "    availability = fileDesc.find(\"{http://www.tei-c.org/ns/1.0}publicationStmt\").find(\"{http://www.tei-c.org/ns/1.0}availability\").find(\"{http://www.tei-c.org/ns/1.0}p\").text\n",
    "    date = fileDesc.find(\"{http://www.tei-c.org/ns/1.0}publicationStmt\").find(\"{http://www.tei-c.org/ns/1.0}date\").text\n",
    "    names = []\n",
    "    for author in fileDesc.findall(\"{http://www.tei-c.org/ns/1.0}sourceDesc\"):\n",
    "        a = author.find(\"{http://www.tei-c.org/ns/1.0}analytic\")\n",
    "        print(a)\n",
    "\n",
    "    print(names)\n",
    "    #print(title)\n",
    "    #print(publisher)\n",
    "    #print(availability)\n",
    "    #print(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b0'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b8'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b11'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b10'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b9'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b2'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b12'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b3'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b6'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b1'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr', 'target': '#b7'}\n",
      "{http://www.tei-c.org/ns/1.0}ref {'type': 'bibr'}\n"
     ]
    }
   ],
   "source": [
    "headings, content = [], []\n",
    "for child in root1.findall(\"{http://www.tei-c.org/ns/1.0}text\"):\n",
    "    body = child.find(\"{http://www.tei-c.org/ns/1.0}body\")\n",
    "    for div in body.findall(\"{http://www.tei-c.org/ns/1.0}div\"):\n",
    "        head = div.find(\"{http://www.tei-c.org/ns/1.0}head\").text\n",
    "        headings.append(head)\n",
    "        para = div.findall(\"{http://www.tei-c.org/ns/1.0}p\")\n",
    "        paraTemp = []\n",
    "        for p in para:\n",
    "            paraTemp.append(p)\n",
    "        content.append(paraTemp)\n",
    "                \n",
    "        for p in div.findall(\"{http://www.tei-c.org/ns/1.0}p/{http://www.tei-c.org/ns/1.0}ref\"):\n",
    "            if p.attrib['type'] == 'bibr':\n",
    "                print(p.tag, p.attrib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = parse_sections(parsed_file)\n",
    "heads = []\n",
    "content = []\n",
    "for section in sections:\n",
    "    heads.append(section['heading'])\n",
    "    content.append(section['text'])\n",
    "\n",
    "paper_sections = pd.DataFrame()\n",
    "paper_sections['heading'] = heads\n",
    "paper_sections['content'] = content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introduction</td>\n",
       "      <td>The rapid increase in research article submiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Related Work</td>\n",
       "      <td>Artificial Intelligence in academic peer revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Description and Analysis</td>\n",
       "      <td>The PeerRead dataset consists of papers, a set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Methodology</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pre-processing</td>\n",
       "      <td>At the very beginning, we convert the papers i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         heading  \\\n",
       "0                   Introduction   \n",
       "1                   Related Work   \n",
       "2  Data Description and Analysis   \n",
       "3                    Methodology   \n",
       "4                 Pre-processing   \n",
       "\n",
       "                                             content  \n",
       "0  The rapid increase in research article submiss...  \n",
       "1  Artificial Intelligence in academic peer revie...  \n",
       "2  The PeerRead dataset consists of papers, a set...  \n",
       "3                                                     \n",
       "4  At the very beginning, we convert the papers i...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_sections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = parse_references(parsed_file)\n",
    "titles = []\n",
    "journals = []\n",
    "authors = []\n",
    "for cit in citations:\n",
    "    titles.append(cit['title'])\n",
    "    journals.append(cit['journal'])\n",
    "    authors.append(cit['authors'])\n",
    "\n",
    "citations_data = pd.DataFrame()\n",
    "citations_data['title'] = titles\n",
    "citations_data['journal'] = journals\n",
    "citations_data['authors'] = authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reliability of reviewers' ratings when using p...</td>\n",
       "      <td>Learned Publishing</td>\n",
       "      <td>Lutz Bornmann; H-D Daniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Universal Sentence Encoder for English</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "      <td>Daniel Cer; Yinfei Yang; Sheng-Yi Kong; Nan Hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The toronto paper matching system: an automate...</td>\n",
       "      <td></td>\n",
       "      <td>Laurent Charlin; Richard Zemel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Investigating domain features for scope detect...</td>\n",
       "      <td></td>\n",
       "      <td>Tirthankar Ghosal; Ravi Sonam; Sriparna Saha; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investigating Impact Features in Editorial Pre...</td>\n",
       "      <td>ACM Press</td>\n",
       "      <td>Tirthankar Ghosal; Rajeev Verma; Asif Ekbal; S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Reliability of reviewers' ratings when using p...   \n",
       "1             Universal Sentence Encoder for English   \n",
       "2  The toronto paper matching system: an automate...   \n",
       "3  Investigating domain features for scope detect...   \n",
       "4  Investigating Impact Features in Editorial Pre...   \n",
       "\n",
       "                                     journal  \\\n",
       "0                         Learned Publishing   \n",
       "1  Association for Computational Linguistics   \n",
       "2                                              \n",
       "3                                              \n",
       "4                                  ACM Press   \n",
       "\n",
       "                                             authors  \n",
       "0                          Lutz Bornmann; H-D Daniel  \n",
       "1  Daniel Cer; Yinfei Yang; Sheng-Yi Kong; Nan Hu...  \n",
       "2                     Laurent Charlin; Richard Zemel  \n",
       "3  Tirthankar Ghosal; Ravi Sonam; Sriparna Saha; ...  \n",
       "4  Tirthankar Ghosal; Rajeev Verma; Asif Ekbal; S...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_sentence  = []\n",
    "ref_citation = []\n",
    "ref_venue = []\n",
    "\n",
    "sentences = ref_sentences(parsed_file)\n",
    "for sentence in sentences:\n",
    "    if sentence['ref_id'] is not None:\n",
    "        sentence['ref_id'] = int(sentence['ref_id'].replace('#b', ''))\n",
    "\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    ref_sentence.append(sentences[i]['ref_sentences'][0])\n",
    "    if sentences[i]['ref_id'] is not None:\n",
    "        index = sentences[i]['ref_id']\n",
    "        ref_citation.append(citations[index]['title'])\n",
    "    else:\n",
    "        ref_citation.append(None)\n",
    "    ref_venue.append(citations[index]['journal'])\n",
    "        \n",
    "ref_sentence_data = pd.DataFrame()\n",
    "ref_sentence_data['ref sentence'] = ref_sentence\n",
    "ref_sentence_data['ref citation'] = ref_citation\n",
    "ref_sentence_data['ref venue'] = ref_venue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ref_id': 0,\n",
       "  'ref_text': '(Bornmann and Daniel, 2010)',\n",
       "  'ref_sentences': ['The quality, randomness, bias, inconsistencies in peer reviews is well-debated across the academic community (Bornmann and Daniel, 2010)']},\n",
       " {'ref_id': 8,\n",
       "  'ref_text': '(Langford and Guzdial, 2015)',\n",
       "  'ref_sentences': [' To study the arbitrariness inherent in the existing peer review system, organisers of the NIPS 2014 conference assigned 10% submissions to two different sets of reviewers and observed that the two committees disagreed for more than quarter of the papers (Langford and Guzdial, 2015)']},\n",
       " {'ref_id': 11,\n",
       "  'ref_text': '(Smith, 2006)',\n",
       "  'ref_sentences': [' Many are of the opinion that the existing peer review system is fragile as it only depends on the view of a selected few (Smith, 2006)']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': '(Kang et al., 2018)',\n",
       "  'ref_sentences': ['The PeerRead dataset (Kang et  , 2018) is an excellent resource towards research and study on this very impactful and crucial problem']},\n",
       " {'ref_id': 10,\n",
       "  'ref_text': 'Price and Flach (2017)',\n",
       "  'ref_sentences': [' Price and Flach (2017) did a thorough study of the various means of computational support to the peer review system']},\n",
       " {'ref_id': 9,\n",
       "  'ref_text': 'Mrowinski et al. (2017)',\n",
       "  'ref_sentences': [' Mrowinski et   (2017) explored an evolutionary algorithm to improve editorial strategies in peer review']},\n",
       " {'ref_id': 2,\n",
       "  'ref_text': '(Charlin and Zemel, 2013)',\n",
       "  'ref_sentences': [' The famous Toronto Paper Matching system (Charlin and Zemel, 2013) was developed to match paper with reviewers']},\n",
       " {'ref_id': None,\n",
       "  'ref_text': '(Ghosal et al., 2018b,a)',\n",
       "  'ref_sentences': [' Recently we (Ghosal et  , 2018b,a) investigated the impact of various fea-tures in the editorial pre-screening process']},\n",
       " {'ref_id': 12,\n",
       "  'ref_text': 'Wang and Wan (2018)',\n",
       "  'ref_sentences': [' Wang and Wan (2018) explored a multi-instance learning framework for sentiment analysis from the peer review texts']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': '(Kang et al., 2018)',\n",
       "  'ref_sentences': [' We carry our current investigations on a portion of the recently released PeerRead dataset (Kang et  , 2018)']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' Our approach achieves significant performance improvement over the two tasks defined in Kang et   (2018)']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' For more details on the dataset creation and the task, we request the readers to refer to Kang et   (2018)']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' One motivation of our work stems from the finding that aspect scores for certain factors like Impact, Originality, Soundness/Correctness which are seemingly central to the merit of the paper, often have very low correlation with the final recommendation made by the reviewers as is made evident in Kang et   (2018)']},\n",
       " {'ref_id': 3,\n",
       "  'ref_text': '(Ghosal et al., 2018a)',\n",
       "  'ref_sentences': [' This also seconds our recent finding that determining the scope or appropriateness of an article to a venue is the first essential step in peer review (Ghosal et  , 2018a)']},\n",
       " {'ref_id': 6,\n",
       "  'ref_text': '(Hutto and Gilbert, 2014)',\n",
       "  'ref_sentences': [' To calculate the sentiment polarity of a review text, we take the average of the sentence wise sentiment scores from Valence Aware Dictionary and sEntiment Reasoner (VADER) (Hutto and Gilbert, 2014)']},\n",
       " {'ref_id': 1,\n",
       "  'ref_text': '(Cer et al., 2018)',\n",
       "  'ref_sentences': ['com/allenai/science-parse Encoder (USE) (Cer et  , 2018), d is the dimension of the sentence semantic vector which is 512']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': ['To compare with Kang et   (2018), we keep the experimental setup (train vs test ratio) identical and re-implement their codes to generate the comparing figures']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' However, Kang et   (2018) performed Task 2 on ICLR 2017 dataset with handcrafted features, and Task 1 in a deep learning setting']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' With only using review+sentiment information, we are still able to outperform Kang et   (2018) by a margin of 11% in terms of RMSE']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': ['For Task 2, we observe that the handcrafted feature-based system by Kang et   (2018) performs inferior compared to the baselines']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' The reason is that the work reported in Kang et   (2018) relies on elementary handcrafted features extracted only from the paper; does not consider the review features whereas we include the review features along with the sentiment information in our deep neural architecture']},\n",
       " {'ref_id': 7,\n",
       "  'ref_text': 'Kang et al. (2018)',\n",
       "  'ref_sentences': [' However, we also find that our approach with only Review+Sentiment performs inferior to the Paper+Review method in Kang et   (2018) for ACL 2017']},\n",
       " {'ref_id': None,\n",
       "  'ref_text': '5',\n",
       "  'ref_sentences': ['Figure 3 shows the output activations 5 from the final layer of MLP Senti against the predicted recommendation scores']}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref sentence</th>\n",
       "      <th>ref citation</th>\n",
       "      <th>ref venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quality, randomness, bias, inconsistencies...</td>\n",
       "      <td>Reliability of reviewers' ratings when using p...</td>\n",
       "      <td>Learned Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To study the arbitrariness inherent in the ex...</td>\n",
       "      <td>The arbitrariness of reviews, and advice for s...</td>\n",
       "      <td>Communications of the ACM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Many are of the opinion that the existing pee...</td>\n",
       "      <td>Peer review: a flawed process at the heart of ...</td>\n",
       "      <td>Journal of the Royal Society of Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The PeerRead dataset (Kang et  , 2018) is an e...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price and Flach (2017) did a thorough study o...</td>\n",
       "      <td>Computational support for academic peer review...</td>\n",
       "      <td>Commun. ACM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mrowinski et   (2017) explored an evolutionar...</td>\n",
       "      <td>Artificial intelligence in peer review: How ca...</td>\n",
       "      <td>PloS one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The famous Toronto Paper Matching system (Cha...</td>\n",
       "      <td>The toronto paper matching system: an automate...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recently we (Ghosal et  , 2018b,a) investigat...</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wang and Wan (2018) explored a multi-instance...</td>\n",
       "      <td>Sentiment Analysis of Peer Review Texts for Sc...</td>\n",
       "      <td>ACM Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We carry our current investigations on a port...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Our approach achieves significant performance...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>For more details on the dataset creation and ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>One motivation of our work stems from the fin...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This also seconds our recent finding that det...</td>\n",
       "      <td>Investigating domain features for scope detect...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>To calculate the sentiment polarity of a revi...</td>\n",
       "      <td>VADER: A parsimonious rule-based model for sen...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>com/allenai/science-parse Encoder (USE) (Cer e...</td>\n",
       "      <td>Universal Sentence Encoder for English</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>To compare with Kang et   (2018), we keep the ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>However, Kang et   (2018) performed Task 2 on...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>With only using review+sentiment information,...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For Task 2, we observe that the handcrafted fe...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The reason is that the work reported in Kang ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>However, we also find that our approach with ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Figure 3 shows the output activations 5 from t...</td>\n",
       "      <td>None</td>\n",
       "      <td>Association for Computational Linguistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ref sentence  \\\n",
       "0   The quality, randomness, bias, inconsistencies...   \n",
       "1    To study the arbitrariness inherent in the ex...   \n",
       "2    Many are of the opinion that the existing pee...   \n",
       "3   The PeerRead dataset (Kang et  , 2018) is an e...   \n",
       "4    Price and Flach (2017) did a thorough study o...   \n",
       "5    Mrowinski et   (2017) explored an evolutionar...   \n",
       "6    The famous Toronto Paper Matching system (Cha...   \n",
       "7    Recently we (Ghosal et  , 2018b,a) investigat...   \n",
       "8    Wang and Wan (2018) explored a multi-instance...   \n",
       "9    We carry our current investigations on a port...   \n",
       "10   Our approach achieves significant performance...   \n",
       "11   For more details on the dataset creation and ...   \n",
       "12   One motivation of our work stems from the fin...   \n",
       "13   This also seconds our recent finding that det...   \n",
       "14   To calculate the sentiment polarity of a revi...   \n",
       "15  com/allenai/science-parse Encoder (USE) (Cer e...   \n",
       "16  To compare with Kang et   (2018), we keep the ...   \n",
       "17   However, Kang et   (2018) performed Task 2 on...   \n",
       "18   With only using review+sentiment information,...   \n",
       "19  For Task 2, we observe that the handcrafted fe...   \n",
       "20   The reason is that the work reported in Kang ...   \n",
       "21   However, we also find that our approach with ...   \n",
       "22  Figure 3 shows the output activations 5 from t...   \n",
       "\n",
       "                                         ref citation  \\\n",
       "0   Reliability of reviewers' ratings when using p...   \n",
       "1   The arbitrariness of reviews, and advice for s...   \n",
       "2   Peer review: a flawed process at the heart of ...   \n",
       "3   A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "4   Computational support for academic peer review...   \n",
       "5   Artificial intelligence in peer review: How ca...   \n",
       "6   The toronto paper matching system: an automate...   \n",
       "7                                                None   \n",
       "8   Sentiment Analysis of Peer Review Texts for Sc...   \n",
       "9   A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "10  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "11  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "12  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "13  Investigating domain features for scope detect...   \n",
       "14  VADER: A parsimonious rule-based model for sen...   \n",
       "15             Universal Sentence Encoder for English   \n",
       "16  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "17  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "18  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "19  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "20  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "21  A Dataset of Peer Reviews (PeerRead): Collecti...   \n",
       "22                                               None   \n",
       "\n",
       "                                    ref venue  \n",
       "0                          Learned Publishing  \n",
       "1                   Communications of the ACM  \n",
       "2    Journal of the Royal Society of Medicine  \n",
       "3   Association for Computational Linguistics  \n",
       "4                                 Commun. ACM  \n",
       "5                                    PloS one  \n",
       "6                                              \n",
       "7                                              \n",
       "8                                   ACM Press  \n",
       "9   Association for Computational Linguistics  \n",
       "10  Association for Computational Linguistics  \n",
       "11  Association for Computational Linguistics  \n",
       "12  Association for Computational Linguistics  \n",
       "13                                             \n",
       "14                                             \n",
       "15  Association for Computational Linguistics  \n",
       "16  Association for Computational Linguistics  \n",
       "17  Association for Computational Linguistics  \n",
       "18  Association for Computational Linguistics  \n",
       "19  Association for Computational Linguistics  \n",
       "20  Association for Computational Linguistics  \n",
       "21  Association for Computational Linguistics  \n",
       "22  Association for Computational Linguistics  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_sentence_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PeerRead dataset (Kang et  , 2018) is an excellent resource towards research and study on this very impactful and crucial problem\n",
      "A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications\n"
     ]
    }
   ],
   "source": [
    "sample_sen = ref_sentence[3]\n",
    "sample_cit = ref_citation[3]\n",
    "\n",
    "print(sample_sen)\n",
    "print(sample_cit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_Similarity(doc1, doc2):\n",
    "    \n",
    "    # List the unique words in a document\n",
    "    words_doc1 = set(doc1.lower().split()) \n",
    "    words_doc2 = set(doc2.lower().split())\n",
    "    \n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = words_doc1.union(words_doc2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jaccard_Similarity(sample_sen, sample_cit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer, util\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = '/home/siddhant/dependancies/stsb-roberta-large/'\n",
    "#model = SentenceTransformer(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Dataset of Peer Reviews (PeerRead): Collection, Insights and NLP Applications'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The PeerRead dataset (Kang et  , 2018) is an excellent resource towards research and study on this very impactful and crucial problem'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8528771485274309\n"
     ]
    }
   ],
   "source": [
    "sen1 = nlp(sample_sen)\n",
    "sen2 = nlp(sample_cit)\n",
    "\n",
    "print(sen1.similarity(sen2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanticScore(sentence1, sentence2):\n",
    "    # Tokenize the sentences\n",
    "    doc1 = nlp(sentence1)\n",
    "    doc2 = nlp(sentence2)\n",
    "    \n",
    "    # Find the similarity score using the cosine similarity function\n",
    "    return doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sentence = ref_sentence\n",
    "ref_citation = ref_citation\n",
    "ref_semantic = []\n",
    "ref_lexical = []\n",
    "\n",
    "for i in range(len(ref_sentence)):\n",
    "    if ref_citation[i] is not None:\n",
    "        ref_semantic.append(semanticScore(ref_sentence[i], ref_citation[i]))\n",
    "        ref_lexical.append(Jaccard_Similarity(ref_sentence[i], ref_citation[i]))\n",
    "    else:\n",
    "        ref_semantic.append(None)\n",
    "        ref_lexical.append(None)\n",
    "\n",
    "similarity = pd.DataFrame()\n",
    "similarity['ref sentence'] = ref_sentence\n",
    "similarity['ref citation'] = ref_citation\n",
    "similarity['semantic score'] = ref_semantic\n",
    "similarity['lexical score'] = ref_lexical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref sentence</th>\n",
       "      <th>ref citation</th>\n",
       "      <th>semantic score</th>\n",
       "      <th>lexical score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quality, randomness, bias, inconsistencies...</td>\n",
       "      <td>Reliability of reviewers' ratings when using p...</td>\n",
       "      <td>0.810094</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To study the arbitrariness inherent in the ex...</td>\n",
       "      <td>The arbitrariness of reviews, and advice for s...</td>\n",
       "      <td>0.882402</td>\n",
       "      <td>0.128205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Many are of the opinion that the existing pee...</td>\n",
       "      <td>Peer review: a flawed process at the heart of ...</td>\n",
       "      <td>0.893017</td>\n",
       "      <td>0.129032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The PeerRead dataset (Kang et  , 2018) is an e...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.852877</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Price and Flach (2017) did a thorough study o...</td>\n",
       "      <td>Computational support for academic peer review...</td>\n",
       "      <td>0.878320</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mrowinski et   (2017) explored an evolutionar...</td>\n",
       "      <td>Artificial intelligence in peer review: How ca...</td>\n",
       "      <td>0.865005</td>\n",
       "      <td>0.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The famous Toronto Paper Matching system (Cha...</td>\n",
       "      <td>The toronto paper matching system: an automate...</td>\n",
       "      <td>0.810994</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Recently we (Ghosal et  , 2018b,a) investigat...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wang and Wan (2018) explored a multi-instance...</td>\n",
       "      <td>Sentiment Analysis of Peer Review Texts for Sc...</td>\n",
       "      <td>0.835049</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We carry our current investigations on a port...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.803780</td>\n",
       "      <td>0.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Our approach achieves significant performance...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.769054</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>For more details on the dataset creation and ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.818640</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>One motivation of our work stems from the fin...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.822257</td>\n",
       "      <td>0.018519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This also seconds our recent finding that det...</td>\n",
       "      <td>Investigating domain features for scope detect...</td>\n",
       "      <td>0.822155</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>To calculate the sentiment polarity of a revi...</td>\n",
       "      <td>VADER: A parsimonious rule-based model for sen...</td>\n",
       "      <td>0.824593</td>\n",
       "      <td>0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>com/allenai/science-parse Encoder (USE) (Cer e...</td>\n",
       "      <td>Universal Sentence Encoder for English</td>\n",
       "      <td>0.642957</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>To compare with Kang et   (2018), we keep the ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.787635</td>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>However, Kang et   (2018) performed Task 2 on...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.810739</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>With only using review+sentiment information,...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.798233</td>\n",
       "      <td>0.064516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>For Task 2, we observe that the handcrafted fe...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.783255</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The reason is that the work reported in Kang ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.829291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>However, we also find that our approach with ...</td>\n",
       "      <td>A Dataset of Peer Reviews (PeerRead): Collecti...</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Figure 3 shows the output activations 5 from t...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ref sentence  \\\n",
       "0   The quality, randomness, bias, inconsistencies...   \n",
       "1    To study the arbitrariness inherent in the ex...   \n",
       "2    Many are of the opinion that the existing pee...   \n",
       "3   The PeerRead dataset (Kang et  , 2018) is an e...   \n",
       "4    Price and Flach (2017) did a thorough study o...   \n",
       "5    Mrowinski et   (2017) explored an evolutionar...   \n",
       "6    The famous Toronto Paper Matching system (Cha...   \n",
       "7    Recently we (Ghosal et  , 2018b,a) investigat...   \n",
       "8    Wang and Wan (2018) explored a multi-instance...   \n",
       "9    We carry our current investigations on a port...   \n",
       "10   Our approach achieves significant performance...   \n",
       "11   For more details on the dataset creation and ...   \n",
       "12   One motivation of our work stems from the fin...   \n",
       "13   This also seconds our recent finding that det...   \n",
       "14   To calculate the sentiment polarity of a revi...   \n",
       "15  com/allenai/science-parse Encoder (USE) (Cer e...   \n",
       "16  To compare with Kang et   (2018), we keep the ...   \n",
       "17   However, Kang et   (2018) performed Task 2 on...   \n",
       "18   With only using review+sentiment information,...   \n",
       "19  For Task 2, we observe that the handcrafted fe...   \n",
       "20   The reason is that the work reported in Kang ...   \n",
       "21   However, we also find that our approach with ...   \n",
       "22  Figure 3 shows the output activations 5 from t...   \n",
       "\n",
       "                                         ref citation  semantic score  \\\n",
       "0   Reliability of reviewers' ratings when using p...        0.810094   \n",
       "1   The arbitrariness of reviews, and advice for s...        0.882402   \n",
       "2   Peer review: a flawed process at the heart of ...        0.893017   \n",
       "3   A Dataset of Peer Reviews (PeerRead): Collecti...        0.852877   \n",
       "4   Computational support for academic peer review...        0.878320   \n",
       "5   Artificial intelligence in peer review: How ca...        0.865005   \n",
       "6   The toronto paper matching system: an automate...        0.810994   \n",
       "7                                                None             NaN   \n",
       "8   Sentiment Analysis of Peer Review Texts for Sc...        0.835049   \n",
       "9   A Dataset of Peer Reviews (PeerRead): Collecti...        0.803780   \n",
       "10  A Dataset of Peer Reviews (PeerRead): Collecti...        0.769054   \n",
       "11  A Dataset of Peer Reviews (PeerRead): Collecti...        0.818640   \n",
       "12  A Dataset of Peer Reviews (PeerRead): Collecti...        0.822257   \n",
       "13  Investigating domain features for scope detect...        0.822155   \n",
       "14  VADER: A parsimonious rule-based model for sen...        0.824593   \n",
       "15             Universal Sentence Encoder for English        0.642957   \n",
       "16  A Dataset of Peer Reviews (PeerRead): Collecti...        0.787635   \n",
       "17  A Dataset of Peer Reviews (PeerRead): Collecti...        0.810739   \n",
       "18  A Dataset of Peer Reviews (PeerRead): Collecti...        0.798233   \n",
       "19  A Dataset of Peer Reviews (PeerRead): Collecti...        0.783255   \n",
       "20  A Dataset of Peer Reviews (PeerRead): Collecti...        0.829291   \n",
       "21  A Dataset of Peer Reviews (PeerRead): Collecti...        0.766495   \n",
       "22                                               None             NaN   \n",
       "\n",
       "    lexical score  \n",
       "0        0.035714  \n",
       "1        0.128205  \n",
       "2        0.129032  \n",
       "3        0.066667  \n",
       "4        0.160000  \n",
       "5        0.130435  \n",
       "6        0.238095  \n",
       "7             NaN  \n",
       "8        0.300000  \n",
       "9        0.115385  \n",
       "10       0.000000  \n",
       "11       0.076923  \n",
       "12       0.018519  \n",
       "13       0.052632  \n",
       "14       0.088235  \n",
       "15       0.100000  \n",
       "16       0.030303  \n",
       "17       0.103448  \n",
       "18       0.064516  \n",
       "19       0.000000  \n",
       "20       0.000000  \n",
       "21       0.000000  \n",
       "22            NaN  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f84e466cbbd769c9510f73e6e777948602437ea16fe736bfda306132e69e312c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
